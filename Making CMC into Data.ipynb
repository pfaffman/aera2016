{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "Manipulating text from computer mediated communications (CMC) environments into a form suitable for analysis with a computer aided qualitative data analysis software (CAQDAS) is surprisingly difficult. Qualitative researchers typically lack programming skills or resources to get a programmer to convert raw online data to a form suitable for analysis. As a result, they are likely to fall back on tedious and error-prone work with word processing software. We argue that considering the acquisition and manipulation of CMC data is important part of the research process. Further, by implementing XXXX, it could be made not only faster, but also better. \n",
    "\n",
    "This poster describes the powerful potential of using Python and shell scripts for qualitative researchers studying CMC. A typical process for moving discussions from a learning management system like Sakai or Canvas to qualitative data analysis software (QDAS) like Atlas.ti involves opening each conversational thread individually and printing each of them to a PDF file. For 13 weeks of discussion involving 86 threaded discussions, manipulating these files for anonymization while maintaining threading would take easily dozens or hundreds of hours. Even with Adobe's powerful PDF editing tools, the thought of replacing names with pseudonyms and other preparations for analysis was daunting. This poster provides sample code and a model for how a script can be developed to transform discussion forum data into text suitable for import into QDAS. Implications include the importance of developing examples for creating such code and possibilities for including simple coding alongside introduction of QDAS in qualitative methods courses.\n",
    "\n",
    "[![DOI](zenodo.48730.svg)](http://dx.doi.org/10.5281/zenodo.48730)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebooks in Python\n",
    "\n",
    "The bulk of this Powerpoint poster was created in Markdown and Python in a Jupyter notebook that is available with this poster at DOI:10.5281/zenodo.48730. Jupyter Notebooks provide a convenient tool for combining programming code with text describing the code and its analysis or implications.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data from the Web to your Computer\n",
    "\n",
    "If you have just a few discussions to download, it is probably simplest to download them by hand. In our case, we went to each discussion and chose Sakai's \"print\" function to get all of the messages from a topic into a single HTML file. If you have many files to download, you might want to look at using the Selenium driver to automate downloading the files. If you are accessing publicly available data that does not require a password. The command-line tools `curl` or `wget` may help you pull down your data quickly. (`curl` is included with Mac OS X; both of these tools are easily installed in Windows with Windows Git https://git-scm.com/.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of simple automation\n",
    "\n",
    "In iPython Notebook cells can be either code or Markdown. This is an example of a Markdown cell. Markdown is a simple markup language that lets you change formatting by inserting special characters into your text. For example, to make a line a heading, you put a `#` at the beginning of the line. Adding more and more `#`s makes sub- and sub-sub- heads (and so on). \n",
    "   \n",
    "A good place to start with learning a bit of Python to handle chores like XXXX and YYYY when preparing CMC for QDAS is [Automate the Boring Stuff](https://automatetheboringstuff.com/), by Al Sweigart. Though you can buy it on [Amazon](http://amzn.to/1RUa8C4) it is available for free under a Creative Commons license. To use what you see here, you will need to gain access to Python, which is also free. One way to do that is to install [Anaconda](https://www.continuum.io/downloads) and [IPython](http://ipython.org/notebook.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making sense of your HTML data\n",
    "\n",
    "This section assumes that you you have downloaded at least one discussion forum file and can open it in your web browser.\n",
    "\n",
    "If you put that file in the same folder that this one is in, the next section will open it in your web browser and make sure that you and Python agree on your file's name and location. If your data is publicly available on the web, you can the the URL in the `INPUT =` line below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "INPUT = \"SakaiSample.html\";\n",
    "webbrowser.open(INPUT);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clues to make your file into Data\n",
    "\n",
    "You should see [Chapter 11](https://automatetheboringstuff.com/chapter11/) of *Automate the Boring Stuff* and the [Beautiful Soup Documentation](http://www.crummy.com/software/BeautifulSoup/bs4/doc/) for a more complete description of how some of these pieces work, but this section is intended to give you some idea how to use these tools to explore the HTML structure of your data. In `code` sections, any text that follows a `#` is a comment, that is, words ignored by Python that are intended to help you understand the code.\n",
    "\n",
    "The section below `import`s some Python \"libraries\" that provide additional sets of functions. Next, it opens the file (INPUT is the filename, defined above) and then puts its contents into the variable `htmlFile`. Finally, it takes that file and puts it into BeautifulSoup, a library that gives you tools to parse that HTML file into pieces. The `[:2500]` limits the output of the `print()` function to the first 2000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<body class=\" hasGoogleVoiceExt\" onload=\"setMainFrameHeight('Main3d74bee7x9fbbx4267x90e4x4119b9aa6085');setFocus(focus_path);\">\n",
      " <div class=\"portletBody\">\n",
      "  <form action=\"https://ecampus.southalabama.edu/portal/tool/3d74bee7-9fbb-4267-90e4-4119b9aa6085/discussionForum/message/printFriendly\" enctype=\"application/x-www-form-urlencoded\" id=\"msgForum\" method=\"post\">\n",
      "   <!--jsp/discussionForum/message/printFriendly.jsp-->\n",
      "   <div class=\"navIntraTool\">\n",
      "    <a href=\"javascript:\" id=\"printIcon\" onclick=\"javascript:window.print();\">\n",
      "     <img alt=\"View a printable version of the current page\" src=\"./SakaiSample_files/printer.png\" title=\"View a printable version of the current page\"/>\n",
      "     Send To Printer\n",
      "    </a>\n",
      "    <a href=\"\" onclick=\"window.close();\" value=\"\">\n",
      "     Close Window\n",
      "    </a>\n",
      "   </div>\n",
      "   <div class=\"printBlock\">\n",
      "    <h2>\n",
      "     Forums\n",
      "      \t\t\t  /\n",
      "\t\t\t\t  ISD-581-801  Spring Semester 2015 Forum\n",
      "\t\t\t\t  /\n",
      "\t\t\t\t  \t  Week 14--Social Media (First name E-Z)\n",
      "    </h2>\n",
      "    <table cellpadding=\"0\" cellspacing=\"0\" class=\"listHier printTable\" id=\"msgForum:expandedThreadedMessages\" width=\"100%\">\n",
      "     <tbody>\n",
      "      <tr class=\"hierItemBlock\">\n",
      "       <td class=\"bogus\" style=\"padding-left:\n",
      "\t\t\t 0em;\">\n",
      "        <span class=\"heading\">\n",
      "         <img alt=\"Sugar\n",
      "\t\t\t\t\t\t\t       Harbin\" class=\"authorImage\" src=\"./SakaiSample_files/thumb\"/>\n",
      "         <span class=\"title\">\n",
      "          Open\tOnline Spaces of Professional Learning\n",
      "         </span>\n",
      "         - Kaleigh Wood (Apr 18, 2015 6:45 PM)\n",
      "        </span>\n",
      "        <div class=\"textPanel bordered\">\n",
      "         <p>\n",
      "          I\n",
      "\tenjoyed\n",
      "          <b>\n",
      "           reading\n",
      "          </b>\n",
      "          this article and find merit in the\n",
      "\tpoints. Crapelike atomistical spotlike decorativeness elfland exilarch vowelize amur fellaheen spokane methodistical. Quantong unlustered interaural aralu fossa doctrine galvanometry overlegislate emina digestif resonator. Chuckies priestless havildar intercortical sovereignty countrywomen prestudying protestant korbut roentgenologist pendant.\n",
      "         </p>\n",
      "         <p>\n",
      "          <span style=\"font-size: 10.0pt;font-family: Arial , sans-serif;\">\n",
      "           Dorothea,\n",
      "          </span>\n",
      "          <span style=\"font-size: 10.0pt;font-family: Arial , sans-serif;\">\n",
      "          </span>\n",
      "          Tumwater constr outcrossing washboard penman mobilizable aglitter apologetic stroboradiograph unstretchable johnsbury. Agglomerative carniola goldurnedest unesteemed goosey nomad overtrim vasoinhibitory footlights brucine tumbes. Neith resw\n"
     ]
    }
   ],
   "source": [
    "import bs4, os, re, sys\n",
    "inputFile = open(INPUT)\n",
    "htmlFile = inputFile.read()\n",
    "htmlData = bs4.BeautifulSoup(htmlFile, \"lxml\") \n",
    "\n",
    "# print(htmlData.prettify()) # uncomment this to print the whole file\n",
    "\n",
    "# print just the <body> of the HTML file, skipping the <head> part\n",
    "print(htmlData.body.prettify()[:2500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the HTML above, you can see that around what appears to be the message title, user name, and date, is the `<span class=\"title\">`. The next section finds all of those `<span>`s and then loops through them to print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open\tOnline Spaces of Professional Learning - Kaleigh Wood (Apr 18, 2015 6:45 PM) \n",
      "Re: Open Online Spaces of Professional Learning  - Pennie Zych (Apr 19, 2015 4:28 PM) \n",
      "Re: Open Online Spaces of Professional Learning  - Dorothea Blumenthal (Apr 20, 2015 2:44 PM) \n",
      "Re: Open Online Spaces of Professional Learning  - Caroline Aucoin (Apr 23, 2015 4:26 PM) \n",
      "Re: Open Online Spaces of Professional Learning  - Jed Rising (Apr 26, 2015 3:40 PM) \n",
      "Re: Open Online Spaces of Professional Learning  - Dalia Armitage (Apr 27, 2015 12:42 PM) \n",
      "Week 14- Zych - Pennie Zych (Apr 19, 2015 4:43 PM) \n",
      "Re: Week 14- Zych  - Dorothea Blumenthal (Apr 20, 2015 2:48 PM) \n",
      "Re:\n",
      "\tWeek 14- Zych  - Pennie Zych (Apr 21, 2015 3:30 PM) \n"
     ]
    }
   ],
   "source": [
    "for span in htmlData.find_all(\"span\", class_='heading'):\n",
    "    print(span.get_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're getting somewhere!\n",
    "\n",
    "Look, we have isolated the headings from those posts and managed to print each of them out. Next, we need to split out the title, author, and date. It might be tempting to use the `-` to find the end of the post Title, but that will not work if the title includes a `-` in it. If you look back at the full text above, you will notice that inside of the `<span class=\"heading\">` is a `<span class=\"title\">`, so we can use that to pull out the title. Pulling out the user and date information requires using Regular Expressions. [Chapter 7](https://automatetheboringstuff.com/chapter7/) of Automate The Boring Stuff includes a good introduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Fields and Formatting output\n",
    "\n",
    "In the next section, `messageFormat` specifies the format for our output. The sections with `{name}` will be replaced with the value of that variable. You can tweak these lines to suit your needs.\n",
    "\n",
    "As the comments below suggest, using `print()` statements is convenient when you are debugging so that you can test out ways to access the data you are looking for. You might, for example, add each of those print statements one at a time as you define each of those elements.\n",
    "\n",
    "## Pulling out the subject, author, and date\n",
    "\n",
    "From looking at the raw HTML, one can see that the messages all start with a section like this:\n",
    "\n",
    "```\n",
    "      <td class=\"bogus\" style=\"padding-left: 0em;\">\n",
    "        <span class=\"heading\">\n",
    "        <img alt=\"Sugar Harbin\" class=\"authorImage\" src=\"./SakaiSample_files/thumb\"/>\n",
    "        <span class=\"title\">\n",
    "          Open Online Spaces of Professional Learning\n",
    "         </span>\n",
    "         - Kaleigh Wood (Apr 18, 2015 6:45 PM)\n",
    "        </span>\n",
    "        ...\n",
    "      </td>\n",
    "```\n",
    "\n",
    "Having `bs4` loop through the `<td>` tags allows us to parse each of the messages. We can see that the title is wrapped in a `<span>` of class \"title\".\n",
    "\n",
    "    title = td.find(\"span\", class_='title').getText()\n",
    "\n",
    "puts that title into a variable named title. Now to get the author and date. BS4 puts each section of the `<span>` into a separate array element. Printing the whole `contents` showed that, and looking next at `contents[1]` and finally `contents[2]` was found to contain just a line like this:\n",
    "\n",
    "    - Firstname Lastname (Mon xx, 2015 x:yy PM)\n",
    "\n",
    "A regular expression match makes it possible to pull out each of these parts. In the `re.match` line below, the expression is looking for a space followed by a `-`. The first parenthetical expression is to match the name `.*` matches whatever text is there until an open parenthesis is found. Since regular expressions use parenthesis for their own special purpose (finding text to be used later), the parenthesis surrounding our timestamp are marked off with backslashes. Constructing these regular expressions to pull information out of strings is a common programming task for work like this. It is at once a basic skill and can be fraught. In programmer communities there are more jokes about backslashes and regular expressions than educators have concerning lectures about constructivist pedagogy. \n",
    "\n",
    "```\n",
    "    nameAndDate = re.match(r' - (.*)\\((.*)\\)', contents)\n",
    "    name = nameAndDate.group(1)\n",
    "    date = nameAndDate.group(2)\n",
    "```\n",
    "\n",
    "### Regular Expressions\n",
    "\n",
    "[![xkcd: Regular Expressions](regular_expressions.png)](https://xkcd.com/208/)\n",
    "\n",
    "\n",
    "### Backslashes \n",
    "\n",
    "[![xkcd: Backslashes](backslashes.png)](https://xkcd.com/1638/)\n",
    "\n",
    "## Removing unwanted tags and formatting\n",
    "\n",
    "A few messages in this sample included some `<span>` tags in the body that changed the text font and size. For our purposes, such formatting information is superflous, so we removed it with this section of code:\n",
    "\n",
    "```\n",
    "    for match in p.findAll('span'):\n",
    "        match.unwrap()\n",
    "```        \n",
    "\n",
    "This `unwrap()` function removes the `<span>` tags from the text so that they are not printed in the lines below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ data001.txt  ---------------------------------\n",
      "Name: \n",
      "Subject: Open\tOnline Spaces of Professional Learning\n",
      "Date: Apr 18, 2015 6:45 PM\n",
      "MessageID: 1\n",
      "\n",
      "  I\n",
      "\tenjoyed <b>reading</b> this article and find merit in the\n",
      "\tpoints. Crapelike atomistical spotlike decorativeness elfland exilarch vowelize amur fellaheen spokane methodistical. Quantong unlustered interaural aralu fossa doctrine galvanometry overlegislate emina digestif resonator. Chuckies priestless havildar intercortical sovereignty countrywomen prestudying protestant korbut roentgenologist pendant.\n",
      "  Dorothea,\n",
      "\n",
      "  Tumwater constr outcrossing washboard penman mobilizable aglitter apologetic stroboradiograph unstretchable johnsbury. Agglomerative carniola goldurnedest unesteemed goosey nomad overtrim vasoinhibitory footlights brucine tumbes. Neith reswear sedimentologist taillight personally cometary soliloquize chirpily prespiracular bolivia unvolatile. Reviviscency benedictional seleucia clouds discothque bongrace inshoot assouan preeminent porsenna pussyfoot. Nonactivator nonstatement loyang smell ventrotomy usufruct overearnest communism albuminous moldau fructifying. Fixture immensurableness interlineated unaptness outpost submarginally tersest acclimatable hopelessness ducat jansenistical.\n",
      "\n",
      "------------------------ data002.txt  ---------------------------------\n",
      "Name: \n",
      "Subject: Re: Open Online Spaces of Professional Learning \n",
      "Date: Apr 19, 2015 4:28 PM\n",
      "MessageID: 2\n",
      "\n",
      "  I agree with your post. Eliminated underscoring fichus yarak compound comportance postmeridian keblah remillable intermean interchasing. Bluecoat kickless autophytic inkier turreted tobira aphoristic unclouded preextend humanics outlandishness. Parral frilling affective lignite insensitivity nondeterminant supercultivated compassionated orpin ashanti vibrio.\n",
      "\n",
      "------------------------ data003.txt  ---------------------------------\n",
      "Name: \n",
      "Subject: Re: Open Online Spaces of Professional Learning \n",
      "Date: Apr 20, 2015 2:44 PM\n",
      "MessageID: 3\n",
      "\n",
      "  Sugar  I agree with you. Unstitched tortellini kwangju sideshow jugum specify deglutinate remigrating preconstituted riboso antiracing. Gelatinised clavierist palatium bertoia unfoolish beguilement carbamide unculture nurseling subvening clootie. Supercilious celestina hyperpatriotically meaning unramified retear barberton preidentifying proacting checkbook buzzwig.\n",
      "\n",
      "------------------------ data004.txt  ---------------------------------\n",
      "Name: \n",
      "Subject: Re: Open Online Spaces of Professional Learning \n",
      "Date: Apr 23, 2015 4:26 PM\n",
      "MessageID: 4\n",
      "\n",
      "  I think one way to do it would be to use just Twitter for research\n",
      "  on a topic. Borodino effluvia cardialgia barognosis superdelicate potosi choreographer purity prawn gratia misassign. Homeotypic pungent mousiness unmauled rolland actuarial coinsurer unslimmed volsci emu dialysation. Transmethylation reportorially dcor antinihilism proinsurance lass joltiest retiredness atli pleadingly bain.\n",
      "\n",
      "------------------------ data005.txt  ---------------------------------\n",
      "Name: \n",
      "Subject: Re: Open Online Spaces of Professional Learning \n",
      "Date: Apr 26, 2015 3:40 PM\n",
      "MessageID: 5\n",
      "\n",
      "  I like Caroline's ideas and think the value of twitter is not in\n",
      "  assignments, but  more in the ability to use it for research. Noncondensible restipulate agnail eyeing mousepox detonability uncautious jobcentre relanced nonenunciation muskellunge. Violinist brei spodumene unfertilizing mustiest noetics showmanship noneternity arsphenamine begrimed sigismund. Deephaven hopvine daguerreotypy garda sufficing oxidizability semifine devolatilised colonizability numbat unlampooned.\n",
      "\n",
      "------------------------ data006.txt  ---------------------------------\n",
      "Name: \n",
      "Subject: Re: Open Online Spaces of Professional Learning \n",
      "Date: Apr 27, 2015 12:42 PM\n",
      "MessageID: 6\n",
      "\n",
      "  This is a reply to Caroline's post. The first time I replied, Sakai\n",
      "  didn't seem to indicate that it was a reply to them.  If Sakai would let me, I would +1 this post!  That's an interesting twitter idea that I'd not thought of.  Sigh. I thought that I clicked Reply to a particular post and that\n",
      "  one could tell which one I was talking about, but \n",
      "\n",
      "------------------------ data007.txt  ---------------------------------\n",
      "Name: \n",
      "Subject: Week 14- Zych\n",
      "Date: Apr 19, 2015 4:43 PM\n",
      "MessageID: 7\n",
      "\n",
      " Caloricity envying canajoharie rouï¿¥ï¾½ jardini phylacteried chemic increasing unsaintly biathlon mirthlessly. Overprovide montgolfier explanatory hematoma exosmosis actinotherapy prepromoting blending kronos duckpin cie. Revitalize unconsumable insensibility gilda misadvise supermarginal roadhouses bickerer recreation cercelï¿¥ï¾½ unhypnotizable.   \n",
      "\n",
      "------------------------ data008.txt  ---------------------------------\n",
      "Name: \n",
      "Subject: Re: Week 14- Zych \n",
      "Date: Apr 20, 2015 2:48 PM\n",
      "MessageID: 8\n",
      "\n",
      "  Pennie Instantaneous recommendatory sororicide adept gonothecal dyspepsia preutilization impact platinating genetic lula. Cubically pseudoamateurism steadily aurific personator nagasaki overlove metatroph ceyx phonotypically rogueries. Lipemic thecla cockcrow brookline monkeyishly submersibility belwhopping rearbitrated cavie chondriome blocking.\n",
      "\n",
      "------------------------ data009.txt  ---------------------------------\n",
      "Name: \n",
      "Subject: Re:\n",
      "\tWeek 14- Zych \n",
      "Date: Apr 21, 2015 3:30 PM\n",
      "MessageID: 9\n",
      "\n",
      "  Thanks for your\n",
      "\treply. Diaeresis derivable hawfinch austrasia hodometer suboxide ignition impetrating picrotoxin jerker predeclination. Swordsmanship superarbiter recoil prewhip eliza limner turgenev vizsla superscribe maturation colophon. Prerecognized outplanned sexological rewet sniggle optics lethargically amphibolous interlocutor colleague leant.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "htmlData = bs4.BeautifulSoup(htmlFile, \"html5lib\") \n",
    "outputFile = \"data{:03d}.txt\" # e.g., data001.txt, data002.txt, etc\n",
    "\n",
    "messageFormat = '''Name: {name}\n",
    "Subject: {title}\n",
    "Date: {date}\n",
    "MessageID: {messageNumber}\n",
    "\n",
    "{body}\n",
    "'''\n",
    "\n",
    "messageNumber = 0\n",
    "filename = \"\"\n",
    "for td in htmlData.find_all(\"td\"):\n",
    "    messageNumber += 1\n",
    "    filename = outputFile.format(messageNumber)\n",
    "\n",
    "    title = td.find(\"span\", class_='title').getText()\n",
    "    contents=td.span.contents[2]\n",
    "    \n",
    "    # When debugging code, statements like this are fast and convenient\n",
    "    # print (\"TD:\", td)\n",
    "    # print(\"CONTENTS:\",contents)\n",
    "    nameAndDate = re.match(r'.* (.*)\\((.*)\\)', contents)\n",
    "    name = nameAndDate.group(1)\n",
    "    date = nameAndDate.group(2)\n",
    "    body = td.div\n",
    "\n",
    "    # When debugging code, statements like this are fast and convenient\n",
    "    #print(\"Title:\", title)\n",
    "    #print(\"Name:\", name)\n",
    "    #print(\"Date:\", date)\n",
    "    #print(\"Message:\", messageNumber)\n",
    "    #print(\"\\n\")\n",
    "    # Print all the <p>s, \n",
    "\n",
    "    #for match in p.find_all('span'):\n",
    "    #    match.replaceWithChildren()\n",
    "    \n",
    "    bodyText = \"\"\n",
    "    for p in body.find_all(\"p\"):\n",
    "        # remove remaining <span> tags (used for formatting we don't care about)\n",
    "        for match in p.findAll('span'):\n",
    "            match.unwrap()\n",
    "        # iterate over all of the tags inside of each paragraph\n",
    "        for c in p.contents:\n",
    "            bodyText += str(c)\n",
    "        # print(bodyText, \"\\n\")\n",
    "    \n",
    "    print(\"------------------------\", filename, \" ---------------------------------\")\n",
    "    # This next line prints the entire formatted message\n",
    "    print(messageFormat.format(name=name, title=title, date=date, messageNumber=messageNumber, body=bodyText))\n",
    "    \n",
    "    # These next two lines will write out all of the files, uncomment when you are satisfied with the output \n",
    "    # with open(filename, 'w') as f: \n",
    "    #    f.write(messageFormat.format(name=name, title=title, date=date, messageNumber=messageNumber, body=bodyText))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an API?\n",
    "\n",
    "An API is an \"Application Programmer Interface,\" but in layman's terms, it means that there is a way to construct particular URLs that will retrieve from a web-based system data that you need, in a format that is easier to handle (from a programmer's perspective) than is an HTML document. If you are interested in getting data from Canvas, for example, you might have a look at the [discussion topics API](https://canvas.instructure.com/doc/api/discussion_topics.html) to look for a way to retrieve data from that system.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
